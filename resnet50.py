# -*- coding: utf-8 -*-
"""resnet50.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nZGA7DbSt_gV_W0M74ULl7Qy3AKb3M9x
"""

# 图片路径存入csv文件
import os
import pandas as pd
from sklearn.utils import shuffle

train_image_dir = '/content/drive/MyDrive/kaggle/data/images/'
train_dir = '/content/drive/MyDrive/kaggle/train.csv'
save_path = '/content/drive/MyDrive/kaggle/data/'

train_df = pd.read_csv(train_dir)
train_df['path'] = train_df['StudyInstanceUID'].map(lambda x: train_image_dir+x)+'.jpg'
train_df = train_df.drop(columns=['StudyInstanceUID'])
train_df = train_df.sample(frac=1).reset_index(drop=True)
len_df = len(train_df)
print(f"There are {len_df} images of train")
image_paths = []
labels = []

for item in train_df.iterrows():
    img_path = item[1][12]
    label = [item[1][i] for i in range(0,11)]
    image_paths.append(img_path)
    labels.append(label)

print (len(image_paths))
print (len(labels))
csv_file = pd.DataFrame({'image_path' : image_paths, 'label' : labels})
csv_file = shuffle(csv_file)
# csv_file.to_csv('/content/drive/MyDrive/kaggle/data/train.csv', columns=['image_path','label'])

test_image_dir = '/content/drive/MyDrive/kaggle/data/images_test/'
test_dir = '/content/drive/MyDrive/kaggle/sample_submission.csv'

train_df = pd.read_csv(test_dir)
train_df['path'] = train_df['StudyInstanceUID'].map(lambda x: test_image_dir+x)+'.jpg'
# train_df = train_df.sample(frac=1).reset_index(drop=True)
len_df = len(train_df)
print(f"There are {len_df} images of test")
image_paths = train_df['path'].tolist()

print (len(image_paths))
csv_file = pd.DataFrame({'image_path' : image_paths})
# csv_file.to_csv('/content/drive/MyDrive/kaggle/data/test.csv', columns=['image_path'], index=None)

from torch.utils.data import Dataset
import cv2
import pandas as pd
import numpy as np
import logging
import torch


class BasicDataset(Dataset):

    def __init__(self, data_dir, img_size=[250, 250],is_train=True):
        data = pd.read_csv(data_dir)
        self.images_dir = data['image_path']   
        self.image_size = img_size
        self.ids = self.images_dir
        self.is_train = is_train
        if is_train:
            self.labels = data['label']
        else:
            self.uid = data['image_path']
        
        logging.info(f'Creating dataset with {len(self.ids)} examples')

    def __len__(self):
        return len(self.images_dir)

    @classmethod
    def preprocess(self, roi_image, label):
        return roi_image, label

    def __getitem__(self, i):
        image_path = self.images_dir[i]
        roi_image = cv2.imread(image_path)
        roi_image = np.transpose(roi_image, (2, 0, 1)) # 调整通道位置
        if self.is_train:
            label = self.labels[i] # 这里的label是str
            tmp1 = label[1:-1]
            tmp2 = tmp1.split(',')
            label = [int(c) for c in tmp2] # 这里的label是list
            label = np.array(label) 
            train_img, train_label = self.preprocess(roi_image, label)
            return {'image': torch.from_numpy(train_img), 'mask': torch.from_numpy(train_label)}
        else:
            uid = self.uid[i]
            return  {'image': torch.from_numpy(roi_image), 'uid': uid}

import torch
import torch.nn as nn
from torch.hub import load_state_dict_from_url
import ssl

ssl._create_default_https_context = ssl._create_unverified_context

model_urls = {'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth'}

def conv3x3(in_planes, out_planes, stride=1, padding=1):
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=padding, bias=False)


def conv1x1(in_planes, out_planes, stride=1):
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,bias=False)


class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1, downsample=None, norm_layer=None):
        super(Bottleneck, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d

        self.conv1 = conv1x1(inplanes, planes)
        self.bn1 = norm_layer(planes)
        self.conv2 = conv3x3(planes, planes, stride)
        self.bn2 = norm_layer(planes)
        self.conv3 = conv1x1(planes, planes * self.expansion)  # 输入的channel数：planes * self.expansion
        self.bn3 = norm_layer(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        identity = x
        
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)
      
        return out


class ResNet(nn.Module):
    def __init__(self, block, layers, num_class=1000, norm_layer=None):
        super(ResNet, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        self._norm_layer = norm_layer

        self.inplanes = 64

        # conv1 in ppt figure
        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = norm_layer(self.inplanes)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # (1,1)等于GAP
        self.fc = nn.Linear(512 * block.expansion, num_class)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def _make_layer(self, block, planes, blocks, stride=1):
        norm_layer = self._norm_layer
        downsample = None

        if stride != 1 or self.inplanes != planes * block.expansion:
            # 需要调整维度
            downsample = nn.Sequential(
                conv1x1(self.inplanes, planes * block.expansion, stride),
                norm_layer(planes * block.expansion)
            )
        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample, norm_layer))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes, norm_layer=norm_layer))
        return nn.Sequential(*layers)

    def forward(self, x):
        # x.shape = [1, 3, 224, 224]
        x = self.conv1(x)
        # x.shape = [1, 64, 112, 112]
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)
        # x.shape = [1, 64, 56, 56]

        x = self.layer1(x)
        # x.shape = [1, 64, 56, 56]
        x = self.layer2(x)
        # x.shape = [1, 128, 28, 28]
        x = self.layer3(x)
        # x.shape = [1, 256, 14, 14]
        x = self.layer4(x)
        # x.shape = [1, 512, 7, 7]

        x = self.avgpool(x)
        # x.shape = [1, 512, 1, 1]
        x = torch.flatten(x, 1)
        # x.shape = [1, 512]
        x = self.fc(x)
        # x.shape = [1, 1000]

        return x


def _resnet(arch, block, layers, pretrained, progress, **kwargs):
    model = ResNet(block, layers, **kwargs)
    if pretrained:
        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)
        model.load_state_dict(state_dict)
    return model


def resnet50(pretrained=False, progress=True, **kwargs):
    r"""ResNet-50 model from
    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)

class MyModel(ResNet):
    def __init__(self, is_pretrained=True,num_classes=11):
        super(MyModel, self).__init__(block=Bottleneck, layers=[3, 4, 6, 3])
        self.model = resnet50(pretrained=is_pretrained)
        self.num_classes = num_classes
        self.fc = nn.Linear(1000, self.num_classes)
        self.sigmoid = nn.Sigmoid()

    def forward(self,input_batch):
        output = self.model(input_batch)
        output = self.fc(output)
        output = self.sigmoid(output)
        return output

import torch


def get_score(pred_choice,target):
    TP, TN, FN, FP = 0, 0, 0, 0
    TP += ((pred_choice == 1) & (target.data == 1)).cpu().sum()
    TN += ((pred_choice == 0) & (target.data == 0)).cpu().sum()
    FN += ((pred_choice == 0) & (target.data == 1)).cpu().sum()
    FP += ((pred_choice == 1) & (target.data == 0)).cpu().sum()

    p = TP / (TP + FP)
    r = TP / (TP + FN)
    F1 = 2 * r * p / (r + p)
    acc = (TP + TN) / (TP + TN + FP + FN)
    return acc, F1

def eval_net(net, loader, device, n_val,is_pred=False):
    net.eval()
    preds= []
    uids = []
    with tqdm(total=n_val, unit='img', leave=False) as pbar:
        for batch in loader:
            imgs = batch['image']
            imgs = imgs.to(device=device, dtype=torch.float32)
            
            if not is_pred:
                with torch.no_grad():
                    mask_pred = net(imgs)
                true_masks = batch['mask']
                true_masks = true_masks.to(device=device, dtype=torch.float32)
                best_acc = 0
                best_F1 = 0
                for true_mask, pred in zip(true_masks, mask_pred):
                    pred = (pred > 0.5).float()
                    acc, F1 = get_score(pred,true_mask)
                    if acc > best_acc or F1 > best_F1:
                         path = '/content/drive/MyDrive/kaggle/resnet50.pkl'
                         torch.save(net.state_dict(),path)

                pbar.update(imgs.shape[0])
                return best_acc, best_F1

            else:
                uid = batch['uid']
                PATH = '/content/drive/MyDrive/kaggle/resnet50.pkl'
                net.load_state_dict(torch.load(PATH))
                with torch.no_grad():
                    mask_pred = net(imgs)
                    pred = (mask_pred > 0.7).float()
                    pred_list = pred.cpu().numpy().tolist()
                    preds.extend(pred_list)
                    uids.extend(uid)
            pbar.update(imgs.shape[0])
                
    return preds, uids

from torch.utils.data import DataLoader, random_split
import torch
from tqdm import tqdm
import pandas as pd

def main(is_train):

    data_dir = '/content/drive/MyDrive/kaggle/data/train.csv'
    test_dir = '/content/drive/MyDrive/kaggle/data/test.csv'
    val_percent = 0.2
    epochs = 1
    num_classes = 11
    batch_size = 64
    base_lr = 1e-6

    dataset = BasicDataset(data_dir, img_size=[250,250],is_train=True)
    dataset_test = BasicDataset(test_dir, img_size=[250,250],is_train=False)
    
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    n_val = int(len(dataset) * val_percent)
    n_train = len(dataset) - n_val
    n_test = len(dataset_test)
    train, val = random_split(dataset, [n_train, n_val])

    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)
    val_loader = DataLoader(val, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)
    test_loader = DataLoader(dataset_test, batch_size=32, shuffle=False, num_workers=0, pin_memory=True)
    model = MyModel(is_pretrained=True,num_classes=11)
    model.to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=base_lr, betas=(0.9, 0.99))

    bce_criterion = torch.nn.BCEWithLogitsLoss()
    # loss_func = torch.nn.MSELoss()

    if is_train==True:
        global_step = 0
        for epoch in range(epochs):
            model.train()
            epoch_loss = 0
            with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:
                for batch in train_loader:
                    imgs = batch['image']
                    true_masks = batch['mask']
        
                    imgs = imgs.to(device=device, dtype=torch.float32)
                    true_masks = true_masks.to(device=device, dtype=torch.float32)
                    
                    masks_pred = model(imgs)

                    loss = bce_criterion(masks_pred, true_masks)
                    # loss = loss_func(true_masks, masks_pred)
                    epoch_loss += loss.item()
                    pbar.set_postfix(**{'loss (batch)': loss.item()})
        
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                    pbar.update(imgs.shape[0])
                    global_step += 1
                    if global_step % (len(dataset) // (10 * batch_size)) == 0:
                        acc, F1 = eval_net(model, val_loader, device, n_val)
                        print('\n accuracy:{0}      F1:{1}'.format(acc, F1))
    if is_train==False:
        preds,uids = eval_net(model, test_loader, device, n_test,is_pred=True)
        test_data = '/content/drive/MyDrive/kaggle/data/test.csv'
        df = pd.read_csv(test_data)
        df['pred'] = preds
        df['StudyInstanceUID'] = uids
        # 预测向量存在pred里，展开成多列
        df['StudyInstanceUID'] = df['StudyInstanceUID'].map(lambda x:x[-68:-4])
        df['ETT - Abnormal'] = df['pred'].map(lambda x:int(float(x[0])))
        df['ETT - Borderline'] = df['pred'].map(lambda x:int(float(x[1])))
        df['ETT - Normal'] = df['pred'].map(lambda x:int(float(x[2])))
        df['NGT - Abnormal'] = df['pred'].map(lambda x:int(float(x[3])))
        df['NGT - Borderline'] = df['pred'].map(lambda x:int(float(x[4])))
        df['NGT - Incompletely Imaged'] = df['pred'].map(lambda x:int(float(x[5])))
        df['NGT - Normal'] = df['pred'].map(lambda x:int(float(x[6])))
        df['CVC - Abnormal'] = df['pred'].map(lambda x:int(float(x[7])))
        df['CVC - Borderline'] = df['pred'].map(lambda x:int(float(x[8])))
        df['CVC - Normal'] = df['pred'].map(lambda x:int(float(x[9])))
        df['Swan Ganz Catheter Present'] = df['pred'].map(lambda x:int(float(x[10])))
        df2 = df.drop(['pred','image_path'],axis=1) 
        df2.to_csv('/content/drive/MyDrive/kaggle/data/submission.csv',index=None)

    print('\nendding *************')

main(is_train=False)
